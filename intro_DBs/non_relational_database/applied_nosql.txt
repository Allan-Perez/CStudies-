How to manage 5000 transactions per second, with 12000 per second in a peak hour?

DIVIDE AND CONQUER.

Just divide this workload into 3 databases. Scaling out is cool.
Even though traditionally the SQL databases didn't support scale out, more and more 
technologies kept coming out, and non relational DBs solve theis problem. 
Gradually, MySQL did offer this solution as well. 


RDBMS -> Scale up in one machine.
To manage this, amazing research and work have take on. The bandwidth of these systems 
can be monstrously big, like 50 GBps, and so on. This is expensive, and even with that, 
sometimes it's note enough. 

Sharding: having little packages that have the information of the DBs, and some replicas. 
		This will allow to have the ability of simply putting more shards (packages) to my
		DB, that will allow to scale a lot. This means that what are being separated are 
		not the columns of a table, but instead we separate the rows.

		Usually, the architecture of these databases are not limited to the nodes of shards, 
		but instead, we add the master node, which does the work of deal with the shared keys between
		the different Slave Nodes. This architecture is usually hold by a cluster of master node, 
		since the work of that master permits the synchronization of the slave nodes, and if it 
		fails, all the service goes down.

		The Shard nodes deal with the rows of the different tables, and depend on the master node 
		to get the different keys for the rows in order to the partitionated tables to make sense.
		And the informaiton doesn't have to be necessarily repeated between these nodes, but it's 
		useful because if something fails, and we have replicas, it can keep working.

		In these kind of architectures, we don't work with SQL, but instead with something different:
		JSONs. 
		In these architectures, even though it seems strange, the database doesn't "exists" until I 
		have fully loaded the data. With these stuff, that btw is called NoSQL, we don't work with the 
		structures and entities of the SQL, but instead we work with something called collections, 
		and these collections will be in some modules, or shards, which can be distributed. 


How MongoDB works?
	Also works with shards, where each shard is called mongoD, and we have some main structures called Mongos.

	Client is connected to the DB (really the Mongos) via some protocols like JPA (Java Persistance API).
	Equally, we have some Configuration nodes that will store the config of the datbase, giving the data 
	some shape, in order to it to make sense, and can be configured to have major disponibility, 
	scalability or consistency. When doing this, and architecting the shape of the DB, we have to have into
	account the Communications and Networks, and not only my data. This is due to the division of shards, which
	essentially are separated machines, and have to have different ports in order to the drivers modules are able 
	to communicate with each (communicating shards with each other).




Let's clear some concepts:
	- Collections: This is the basic structure of data with which I will work in my NoSQL DB. 
					Collections doesn't have necessarily to be replicated through different nodes. Indeed, we don't need
					a lot of nodes. Indeed, we can work with just 1 node. In this kind of architectures, we store the 
					data in a key-value way. We have a key, and some values are related to that key, plus we give an 
					allocation to that data. So that one allocation can be at shard 5, and other alloc in shard 2. 
					Here we don't allocate one collection to one table (with which we did with SQL), but instead we work 
					with one collection that can belong to one node of the cluster. 

					How this work is that we have a "main" table where we collect where some data is in some shards.
					This architecture has as higher tolerance of failure, so that if one node doesn't work, the DB can 
					still run.

	- RAID 1 Mirror : Having a node A with information "a" and a node B with information "a" is not an easy task.
					You have to have a super ultra high velocity between node A and B in order to the 
					data to sync.

	- HeartBeat backup: Having a node A and a node B, being A the production node, have information "a", and the node
						B that is some kind of backup, has information "a'", that is being updated asynchrounously 
						with node A, and is ready to take the command of the "production" node if A fails. This assures
						high disponibility 

	- Logs: This assures data consistency. Having logs within a determined timespan, how was the data in that time in order
			to make comparisions. This way, if a block says me that at 5:00pm A had 5, and other block says that at 5:00pm 
			A had 4, there's an inconsistency.

	- Snaps: "pics" of the (entire) database, which can be used as a kind of "backup".






In order to analyze the disponibility and scalability of my datbase, I have to have into account the connections between 
nodes, they have to be highly efficient. It may be that the performance of the NoSQL, distributed system of DB is actually 
lower than the traditional relational DataBase IF WE DON'T HAVE INTO ACCOUNT THE CONNECTIVITY. More than 10 GBps may be a 
good point from which to start. 

There are some problems that have to be solved when using this architecture: 
	- Information Consistency:  It have to be taken into account that some nodes can fall appart, and stop working, but 
	even with these fails, the system must be up and running, the infromation being able to be accessed. This si achieved 
	by replicating the infromation throughout some nodes.

	- LAN global.

	- Infiniband (>10GBps)

	- Bash packages. Dont send little by little, but instead accumulate a lot of packages, and then make a massive send.


Variables for the architecture of a NoSQL DB:
	- N: numebr of replicas. In Hadoop-like (or blockchain) architectures it's recommended to have 3 replicas.

	- W: number of succeful replicas ready for update.

	- R: number of Query-able replicas for a lecture.

	W + R > N








COLLECTION examples:
	>> Passanger = {
		Name : "Elon",
		LName: "Musk",
		ID.  : "1234",
		email: "elon@musk.com",
		address: [
			{dir: "Tesla Motors"},
			{dir: "SpaceX"}
		]

Remember that we don't have any collection until we don't make any insert into our collection:

	>> USE Passanger // Insertion
	>> db.data.save(Passanger) // commit -- creation of the DB 

	}